---
title: "Customizing an MCMC"
subtitle: "BayesComp 2020 short course"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
library(methods)  # otherwise new() not being found - weird
library(nimble)
```

# NIMBLE's default MCMC

Here are the results from running NIMBLE's default MCMC:

```{r, litters-default, fig.height=6, fig.width=12, fig.cap=''}
source('chunks_litters.R')
littersConf <- configureMCMC(littersModel, monitors = c('a', 'b', 'p'))
littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel)
niter <- 5000
nburn <- 1000
set.seed(1)
samples <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
        inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)

makePlot(samples)
```

# Customizing samplers: examining the defaults

One of NIMBLE's most important features is that users can easily modify the MCMC algorithm used for their model. The easiest thing to do is to start with NIMBLE's default MCMC and then make modifications. 

```{r default-config}
littersConf$printSamplers()
```

# Customizing samplers: modifying the samplers

```{r customize-mcmc}
hypers <- c('a[1]', 'b[1]', 'a[2]', 'b[2]')
for(h in hypers) {
      littersConf$removeSamplers(h)
      littersConf$addSampler(target = h, type = 'slice')
}
littersConf$printSamplers()

littersMCMC <- buildMCMC(littersConf)
## we need 'resetFunctions' because we are rebuilding the MCMC for an existing model for
## which we've already done some compilation
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel,
   resetFunctions = TRUE)

set.seed(1)
samplesSlice <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

# Customizing samplers: Initial results

We can look at diagnostics and see if the change in samplers had an effect. Interestingly, despite the posterior correlation between ```a[i]``` and ```b[i]```, a simple change just to the univariate samplers for the four hyperparameters has had some effect on MCMC performance.

Caveat: the real question is the effective sample size per unit of computation time (each slice sampler iteration is slower than each Metropolis iteration), but we don't assess that at the moment.


```{r output-slice, fig.height=6, fig.width=12, fig.cap=''}
library(coda, warn.conflicts = FALSE)
effectiveSize(samplesSlice)
makePlot(samplesSlice)
```

# Blocking parameters

Often a key factor that reduces MCMC performance is dependence between parameters that limits the ability of univariate samplers to move very far. A standard strategy is to sample correlated parameters in blocks. Unlike many other MCMC engines, NIMBLE makes it easy for users to choose what parameters to sample in blocks.

We'll try that here for ```a``` and ```b```.

```{r customize-mcmc2}
niter <- 5000
nburn <- 1000

littersConf <- configureMCMC(littersModel, monitors = c('a', 'b', 'p'))
hypers <- littersModel$getNodeNames(topOnly = TRUE)
print(hypers)
for(h in hypers) {
      littersConf$removeSamplers(h)
}
littersConf$addSampler(target = c('a[1]','b[1]'), type = 'RW_block', 
                              control = list(adaptInterval = 100))
littersConf$addSampler(target = c('a[2]','b[2]'), type = 'RW_block', 
                              control = list(adaptInterval = 100))

littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)

set.seed(1)
samplesBlock <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```{r output-block, fig.height=6, fig.width=12, fig.cap=''}
effectiveSize(samplesBlock)
makePlot(samplesBlock)
```

The block sampler seems to help some, but hopefully we can do better. Often block sampling gives bigger improvements.

# (Omitted) Implicitly integrating over the random effects: cross-level sampler


Note that in this model, one could analytically integrate over the random effects (necessarily so since we have conjugacy). In NIMBLE this is pretty easy to do using user-defined distributions (next module), though it requires some technical knowledge of working with distributions.

An easier alternative to analytically integrating over the random effects is to use a computational trick that mathematically achieves the same result.

That is NIMBLE's *cross-level sampler*. Here's what it does:

  - do a blocked Metropolis random walk on one or more hyperparameters and
  - then a conjugate update of the dependent nodes conditional on the proposed hyperparameters,
  - accept/reject everything together

Comments:

  - this amounts to a joint update of the hyperparameters and their dependent nodes
  - equivalent to analytically integrating over the dependent nodes
  - this is the *one-block* sampler in Knorr-Held and Rue (2002)

# (Omitted) Applying the cross-level sampler


```{r, cross-level, fig.height=6, fig.width=12, fig.cap=''}
littersConf$removeSamplers(c('a', 'b', 'p'))
littersConf$addSampler(c('a[1]', 'b[1]'), 'crossLevel')
littersConf$addSampler(c('a[2]', 'b[2]'), 'crossLevel')

littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel,
   resetFunctions = TRUE)

set.seed(1)
samplesCross <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

# (Omitted) Cross-level sampler results

```{r output-cross-level, fig.height=6, fig.width=12, fig.cap=''}
effectiveSize(samplesCross)
makePlot(samplesCross)
```

Much better, though we'd still want to look into the lack of movement for `a[1], b[1]` in the initial non-burnin samples -- this could probably be improved with better initialization of the top-level block sampler. 

